package org.apache.solr.schema;import java.util.Iterator;import org.apache.lucene.analysis.TokenStream;import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;import org.apache.lucene.analysis.tokenattributes.FlagsAttribute;import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;import org.apache.lucene.analysis.tokenattributes.PayloadAttribute;import org.apache.lucene.analysis.tokenattributes.PositionIncrementAttribute;import org.apache.lucene.analysis.tokenattributes.TermToBytesRefAttribute;import org.apache.lucene.analysis.tokenattributes.TypeAttribute;import org.apache.lucene.document.Fieldable;import org.apache.lucene.index.Payload;import org.apache.lucene.util.Attribute;import junit.framework.TestCase;public class TestPreAnalyzedField extends TestCase {    private static final String[] valid = {    "one two three",                       // simple parsing    " one  two   three ",                  // spurious spaces    "one,s=123,e=128,i=22  two three,s=20,e=22",    // attribs    "\\ one\\ \\,,i=22,a=\\, two\\=\n\r\t\\n,\\ =\\   \\", // escape madness    ",i=22 ,i=33,s=2,e=20 , ", // empty token text, non-empty attribs    "=This is the stored part with \\= \n \\n \t \\t escapes.=one two three ", // stored plus token stream    "==", // empty stored, no token stream    "=this is a test.=", // stored + empty token stream    "one,p=deadbeef two,p=0123456789abcdef three" // payloads  };  private static final String[] invalid = {    "o,ne two", // missing escape    "one t=wo", // missing escape    "one,, two", // missing attribs, unescaped comma    "one,s ",   // missing attrib value    "one,s= val", // missing attrib value, unescaped space    "one,s=,val", // unescaped comma    "=", // unescaped equals    "=stored ", // unterminated stored    "===" // empty stored (ok), but unescaped = in token stream  };    SchemaField field = null;  int props =     FieldProperties.INDEXED | FieldProperties.STORED;    public void setUp() {    field = new SchemaField("content", new TextField(), props, null);  }    public void testValid() {    PreAnalyzedField paf = new PreAnalyzedField();    for (String s : valid) {      try {        Fieldable f = paf.fromString(field, s, 1.0f);        StringBuilder sb = new StringBuilder();        String str = f.stringValue();        if (str != null) {          // encode the equals sign          str = str.replaceAll("=", "\\=");          sb.append('=');          sb.append(str);          sb.append('=');        }        TokenStream ts = f.tokenStreamValue();        if (ts != null) {          StringBuilder tok = new StringBuilder();          boolean next = false;          while (ts.incrementToken()) {            if (next) {              sb.append(' ');            } else {              next = true;            }            tok.setLength(0);            Iterator<Class<? extends Attribute>> it = ts.getAttributeClassesIterator();            while (it.hasNext()) {              Class<? extends Attribute> cl = it.next();              Attribute att = ts.getAttribute(cl);              if (cl.isAssignableFrom(CharTermAttribute.class)) {                CharTermAttribute catt = (CharTermAttribute)att;                if (tok.length() > 0) {                  tok.insert(0, PreAnalyzedField.escape(catt.buffer(), catt.length()) + ",");                } else {                  tok.insert(0, PreAnalyzedField.escape(catt.buffer(), catt.length()));                }              } else {                if (cl.isAssignableFrom(TermToBytesRefAttribute.class)) {                  // skip                  continue;                }                if (tok.length() > 0) tok.append(',');                if (cl.isAssignableFrom(FlagsAttribute.class)) {                  tok.append("f=" + Integer.toHexString(((FlagsAttribute)att).getFlags()));                } else if (cl.isAssignableFrom(OffsetAttribute.class)) {                  tok.append("s=" + ((OffsetAttribute)att).startOffset() + ",e=" + ((OffsetAttribute)att).endOffset());                } else if (cl.isAssignableFrom(PayloadAttribute.class)) {                  Payload p = ((PayloadAttribute)att).getPayload();                  if (p == null || p.getData() == null) {                    continue;                  }                  tok.append("p=" + PreAnalyzedField.bytesToHex(p.getData(), p.getOffset(), p.length()));                } else if (cl.isAssignableFrom(PositionIncrementAttribute.class)) {                  tok.append("i=" + ((PositionIncrementAttribute)att).getPositionIncrement());                } else if (cl.isAssignableFrom(TypeAttribute.class)) {                  tok.append("t=" + PreAnalyzedField.escape(((TypeAttribute)att).type()));                } else {                  tok.append(cl.getName() + "=" + PreAnalyzedField.escape(att.toString()));                }              }            }            sb.append(tok);          }        }        ts.reset();        System.out.println(" - toString: '" + sb.toString() + "'");      } catch (Exception e) {        e.printStackTrace();        fail("Should pass: '" + s + "', exception: " + e);      }    }  }    public void testInvalid() {    PreAnalyzedField paf = new PreAnalyzedField();    for (String s : invalid) {      try {        paf.fromString(field, s, 1.0f);        fail("should fail: '" + s + "'");      } catch (Exception e) {        //      }    }  }}